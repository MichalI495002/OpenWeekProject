\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{titlesec} % For customizing section titles
\usepackage{graphicx} % For images
\usepackage{hyperref} % For hyperlinks
\usepackage{fancyhdr} % For fancy headers and footers
\usepackage{lipsum} % For generating dummy text
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{dirtree}
\usepackage{float}
% Set page geometry
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

% Set up hyperref
\hypersetup{
    colorlinks=true,
    allcolors=blue,
    linkcolor=black,
    filecolor=cyan,
    urlcolor=blue,
}

% Customize section titles
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

% Setup headers and footers
\pagestyle{fancy}
\fancyhf{}
\rhead{3D printing defect detection}
\lhead{Data provisioning}
\rfoot{Page \thepage}

% Title and author
\title{{\textbf{Data provisioning for 3D printing defect (spaghetti) detection}\\ {\small Full version}}}
\author{Micha≈Ç Raczkowski}
\date{15-01-2024}

\begin{document}

\maketitle
\thispagestyle{empty} % Remove header/footer for the first page

% Insert table of contents
\newpage
\tableofcontents
\newpage

\setcounter{page}{1} % Start page numbering from here

\section{Introduction}
This document outlines the process of data collection, processing, and labeling for training a YOLO algorithm in detecting defects in 3D printing, with a focus on the "spaghetti" issue.

\section{Data Requirements}
YOLO algorithm require set of labeled images in YOLO format representing the object we wish the algorithm to recognize. 
\smallbreak

\noindent\textbf{Image Data}
\begin{itemize}
    \item Data Type: Image
    \item Minimum resolution: 640 x 480 pixels
    \item Color: Grayscale (black and white)
    \item Format: \verb|.jpg| or \verb|.png| 
    \item Content: The image should feature a 3D printer's nozzle and a partially printed object with "spaghetti" defect on the print.
    \item Composition: The composition is such that the print head is in the upper part of the image, and the printed object is clearly visible with "spaghetti" defect.
    \item Orientation: The orientation should be with the printer oriented for a side view.
    \item Lighting: The lighting should be sufficient and flat, with no harsh shadows.
    \item Dimension: The aspect ratio should be standard, possibly 4:3 or similar.
    \item File Size: The file size should be under one megabyte.
    \item Legal Considerations: Ensure the image is free of copyright or privacy issues.
    \item Metadata: Include capture date, time and settings.
    \item Variety: Some images should be from different angles to provide variety for model 
\end{itemize}

\noindent\textbf{Example:} Image of "spaghetti" defect from side Figure \ref{fig:imageExample} and \ref{fig:imageExampleRotated}
\begin{itemize}
    \item Data Type: Image
    \item Resolution: 640x480 pixels  
    \item Color: Grayscale (black and white)
    \item Format: \verb|.jpg| 
    \item Rotated: 45 degree only Figure \ref{fig:imageExampleRotated}
\end{itemize}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{no_support_57.jpg}
        \caption{"Spaghetti" issue\cite{onlineOpenSource1}}
        \label{fig:imageExample}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{leg_broken_11.jpg}
        \caption{"Spaghetti" issue rotated\cite{onlineOpenSource1}}
        \label{fig:imageExampleRotated}
    \end{subfigure}
    \hfill

    \caption{Example of images}
    \label{fig:test}
\end{figure}


\newpage

\section{Data Collection}
Data is collected from homemade pictures displaying failing prints with certain defect and from online open sources\cite{onlineOpenSource1},\cite{onlineOpenSource2}.

\section{Data Understanding}
In our datasets, we possess a variety of photographs that require filtering to select those most suitable for our model. It's essential to clearly understand our selection criteria while choosing these photos.

\subsection{"Spaghetti" defect recognition}
Because datasets are made out of photos which displays various types of 3D printed defects we have to choose images which displays relevant issue (spaghetti defect) to our use case
\begin{figure}[b]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{no_support_0.jpg}
        \caption{"Spaghetti" issue\cite{onlineOpenSource1}}
        \label{fig:image1}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{24382-129778-20190220-061954.jpg}
        \caption{Layers split issue\cite{onlineOpenSource2}}
        \label{fig:image2}
    \end{subfigure}
    \hfill

    \caption{Two types of 3D printing issues}
    \label{fig:test}
\end{figure}
\\In example images above we can see two types of 3d printing issues.

\begin{itemize}
    \item Figure \ref{fig:image1} we can see issue we are interested in which is "spaghetti" issue it characterize by disorganized, tangled layers of filament, resembling spaghetti, resulting from print errors. 
    \item Figure \ref{fig:image2} we can see issue which we are not interested because it doesn't display "spaghetti" issue which we are focusing on, it displays layers split issue which characterize by distinct gaps and separations between layers, compromising print integrity and appearance
\end{itemize}

From looking on pictures and understanding displayed issues we can come to conclusion that Figure \ref{fig:image1} will be relevant to our dataset, because it displays "spaghetti" issue which we are focusing on. This kind of filtering have to be done by hand, or we can try to find dataset containing already described images.

\subsection{Correct Composition}
Upon analyzing the dataset, it becomes apparent that numerous images exhibit the "spaghetti" defect. Our focus should be on photos showcasing a specific composition - a view of the printer bed with the nozzle above it look Figure \ref{fig:compositionDesirable} This is essential since the software's aim is to detect spaghetti defects during printing. Consequently, we've narrowed down the selection to photos that match the composition outlined in the data requirements. Additionally, we can include a few extra images that solely display the "spaghetti" defect Figure \ref{fig:compositionUndesirable} to teach the model about the general shape of this specific defect.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{no_support_57.jpg}
        \caption{Desirable composition \cite{onlineOpenSource2}}
        \label{fig:compositionDesirable}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{wholespaghetti.jpg}
        \caption{Undesirable composition\cite{onlineOpenSource1}}
        \label{fig:compositionUndesirable}
    \end{subfigure}
    \hfill

    \caption{Comparison of image without and with label}
    \label{fig:test}
\end{figure}

\newpage

\section{Data Processing}
\subsection{Dataset structure}
Dataset we are using is composed of main two important parts:
\begin{itemize}
    \item \textbf{Images}: Display issue of our interest (spaghetti issue)
    
    \item \textbf{Labels}: Set of annotations on an image that define the location (through bounding box coordinates) and type (class information) of each object within the image. 
\end{itemize}
For better understanding here is structure of directories:
\smallbreak
{\dirtree{%
.1 data/.
.2 images/.
.3 train/.
.4 ....
.3 val/.
.4 ....
.2 labels/.
.3 train/.
.4 ....
.3 val/.
.4 ....
}}
\medbreak
\noindent
Directories \verb|train| contains data on which model is trained 
\smallbreak
\noindent
Directories \verb|val| contains data on which model is validated 

\subsection{Change of color}
Since the images in our dataset are colored, we need to convert them into Grayscale. Various tools such as GIMP, Photoshop, and ImageMagick can be employed for this purpose. Alternatively, online tools like ResizePixel {\scriptsize\cite{imagesOnlineTool}} are also an option for this conversion task.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{no_support_57.jpg}
        \caption{Grayscale image \cite{onlineOpenSource2}}
        \label{fig:image1}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{no_support_57_color.jpg}
        \caption{Color image (labeled)\cite{onlineOpenSource2}}
        \label{fig:image2}
    \end{subfigure}
    \hfill

    \caption{Comparison of image without and with label}
    \label{fig:test}
\end{figure}

\subsection{Change of orientation}
To enhance the diversity of our dataset and improve the accuracy of our model, we should consider rotating a portion of our dataset images. Rotating them by 45 degrees is ideal as it significantly alters the image. For this task, tools such as GIMP, Photoshop, and ImageMagick are suitable options. Additionally, online tools like ResizePixel {\scriptsize\cite{imagesOnlineTool}} can also be used for rotating the images.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{no_support_57.jpg}
        \caption{"Spaghetti" issue\cite{onlineOpenSource1}}
        \label{fig:imageExample}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{leg_broken_11.jpg}
        \caption{"Spaghetti" issue rotated\cite{onlineOpenSource1}}
        \label{fig:imageExampleRotated}
    \end{subfigure}
    \hfill

    \caption{Example of images}
    \label{fig:test}
\end{figure}



\subsection{Why we need to add labels to images?}

Labeling images is essential in training the YOLO (You Only Look Once) object detection algorithm for several reasons: 

\begin{itemize}
    \item \textbf{Training Supervision}: Labels provide the algorithm with examples to learn from, indicating what objects are present in the image and where they are.
    \item \textbf{Bounding Box Coordinates}: Labels include bounding box coordinates to teach the model how to locate and size objects in an image. 
    \item \textbf{Class Identification}: Labels assign a class to each object, helping the model to classify objects correctly.
    \item  \textbf{Model Accuracy and Evaluation}: Good quality labels improve the accuracy of the model and allow for effective evaluation of its performance.
    \item \textbf{Algorithm Optimization}:Accurate labels are crucial for minimizing the loss during training and optimizing the algorithm's performance.
\end{itemize}

\newpage

\subsection{Process of labeling}
 There are various tools we can use to label data (e.g. CVAT{ \scriptsize \cite{labelingTool1}}, VoTT{ \scriptsize \cite{labelingTool2}}) but to keep simplicity and be able of rapid prototyping we are using free open source online tool called {\textbf {MakeSense}}{ \scriptsize \cite{labelingTool3}} which is capable enough for our purposes 
 \begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{no_support_62.jpg}
        \caption{no\_support\_62.jpeg \cite{onlineOpenSource2}}
        \label{fig:image1}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LabelByMakeSens.png}
        \caption{no\_support\_62.jpeg (labeled)\cite{onlineOpenSource2}}
        \label{fig:image2}
    \end{subfigure}
    \hfill

    \caption{Comparison of image without and with label}
    \label{fig:test}
\end{figure}
\\Labeling involves identifying and marking the coordinates on an image to pinpoint the location of the defect we aim to detect in our case "spaghetti" defect. Once we have labeled all the images, we can export our labels in the YOLO format, which is specifically designed for the YOLO algorithm. 
\subsection{Dataset final structure}

The final dataset should have this directory structure:
{\dirtree{%
.1 data/.
.2 images/.
.3 train/.
.4 no\_support\_62.jpeg .
.4 ....
.3 val/.
.4 exampleValidationImage.jpeg .
.4 ....
.2 labels/.
.3 train/.
.4 no\_support\_62.txt .
.4 ....
.3 val/.
.4 exampleValidationImage.txt .
.4 ....
}}
We can observe that labels are stored in separate directories, yet they share the same names as their corresponding images. This correlation is essential, as it enables the algorithm to accurately pair each image file with its respective label file. Additionally, it's important to note that the final dataset will contain many images along with their corresponding labels.

\subsection{Label explained}
\noindent
Here is an example of what the contents of a label file might look like: \smallbreak
\verb|0 0.281250 0.764286 0.244643 0.130952| \smallbreak
\noindent
The meaning of sentence above is as follows: 
\begin{itemize}
    \item \verb|0|: number of a class
    \item \verb|0.281250|: X coordinate of center of the box
    \item \verb|0.764286|: Y coordinate of center of the box
    \item \verb|0.244643|: width of the box
    \item \verb|0.130952|: height of the box
\end{itemize}

\section{Conclusion}
After preforming all those task and reading about importance of labeling in YOLO algorithm we should be capable to prepare our own dataset for YOLO algorithm to detect "spaghetti" issue in 3D printing


\begin{thebibliography}{9}

    \bibitem{onlineOpenSource1}
        Dataset: \href{https://www.kaggle.com/datasets/justin900429/3d-printer-defected-dataset}{3D-Printer Defected Dataset}: \\
        {\footnotesize \url{https://www.kaggle.com/datasets/justin900429/3d-printer-defected-dataset}}
    \bibitem{onlineOpenSource2}
        Dataset: \href{https://www.kaggle.com/datasets/mikulhe/3d-printing-errors}{3D printing errors}: \\
        {\footnotesize \url{https://www.kaggle.com/datasets/mikulhe/3d-printing-errors}}
    \bibitem{labelingTool1}
        Computer Vision Annotation Tool: \href{https://www.cvat.ai/}{CVAT}: \\
        {\footnotesize \url{https://www.cvat.ai/}}
    \bibitem{labelingTool2}
        Visual Object Tagging Tool: \href{https://github.com/microsoft/VoTT}{VoTT}: \\
        {\footnotesize \url{https://github.com/microsoft/VoTT}}
    \bibitem{labelingTool3}
        MakeSense: \href{https://www.makesense.ai/}{MakeSense}: \\
        {\footnotesize \url{https://www.makesense.ai/}}
    \bibitem{imagesOnlineTool}
    Resizepixel: \href{https://www.resizepixel.com}{Resizepixel}: \\
        {\footnotesize \url{https://www.resizepixel.com}}

    \end{thebibliography}

\end{document}
